{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6d2189",
   "metadata": {},
   "source": [
    ".ipynbçš„ä»‹é¢æœ€ä¸Šé¢æœ‰ä¸€æ’æ±è¥¿\"+Code\", \"+Markdown\", ...æœ€å¾Œæœ‰å€‹\"Outline\"æ‰“é–‹å°±å¯ä»¥çœ‹åˆ°ç›®éŒ„ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2a977",
   "metadata": {},
   "source": [
    "## é è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # è™•ç†æª”æ¡ˆè·¯å¾‘\n",
    "import numpy as np  # è™•ç†æ•¸å€¼è³‡æ–™\n",
    "import pandas as pd  # è™•ç†è¡¨æ ¼å‹è³‡æ–™\n",
    "from tqdm import tqdm  # è®“è¿´åœˆåŠ ä¸Šé€²åº¦æ¢\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f048e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 2048  # æˆ‘å€‘å¸Œæœ›æ¯ç­†è³‡æ–™é•·åº¦éƒ½ä¸€æ¨£ï¼ˆ2048å€‹æ™‚é–“é»ï¼‰(å¯ä»¥æ”¹)\n",
    "\n",
    "DATA_DIR = \"39_Training_Dataset/39_Training_Dataset/train_data\" # å­˜æ”¾ .txt çš„è³‡æ–™å¤¾\n",
    "INFO_PATH = \"39_Training_Dataset/39_Training_Dataset/train_info.csv\" # æ¨™ç±¤çš„ .csv æª”è·¯å¾‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c1d93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_txt(file_path, seq_len=SEQ_LEN):\n",
    "    data = np.loadtxt(file_path)  # æŠŠ.txtè½‰ç‚ºnumpyé™£åˆ— (T, 6)\n",
    "    \n",
    "    # æ¨™æº–åŒ–è™•ç†ï¼šè®“æ¯å€‹ç¶­åº¦çš„å¹³å‡è®Š0ï¼Œæ¨™æº–å·®è®Š1\n",
    "    data = (data - data.mean(axis=0)) / (data.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # è£åˆ‡æˆ–è£œé›¶ï¼šçµ±ä¸€é•·åº¦ç‚º SEQ_LEN\n",
    "    if data.shape[0] >= seq_len:\n",
    "        data = data[:seq_len]  # è£åˆ‡\n",
    "    else:\n",
    "        pad_width = seq_len - data.shape[0]\n",
    "        data = np.pad(data, ((0, pad_width), (0, 0)), mode='constant')  # np.pad() æ˜¯ NumPy çš„è£œé›¶å‡½å¼\n",
    "        # (0, pad_width) è¡¨ç¤ºå°åˆ—æ•¸ï¼ˆæ™‚é–“é»ï¼‰è£œé›¶åœ¨ã€Œå¾Œé¢ã€\n",
    "        # (0, 0) è¡¨ç¤ºæ¬„ä½æ•¸ï¼ˆ6 å€‹æ„Ÿæ¸¬å™¨ç¶­åº¦ï¼‰ä¸è£œé›¶\n",
    "        # mode='constant' ç”¨ 0 å»è£œ\n",
    "    \n",
    "    return data  # shape = (2048, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "889fd441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1955 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1955/1955 [00:11<00:00, 173.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# è®€å–æ¨™ç±¤ CSV\n",
    "info_df = pd.read_csv(INFO_PATH)\n",
    "\n",
    "# é è™•ç† labelï¼ˆå¾ 0 é–‹å§‹ï¼‰\n",
    "info_df['gender'] -= 1  # gender: 1,2 â†’ 0,1\n",
    "info_df['hand'] = info_df['hold racket handed'] - 1  # 1,2 â†’ 0,1\n",
    "info_df['level'] -= 2  # level: 2~5 â†’ 0~3\n",
    "\n",
    "# å»ºç«‹ç©º list ä¾†å­˜è™•ç†å¾Œçš„è³‡æ–™\n",
    "X_data = []  # æ¯ç­† (2048, 6) çš„æ™‚é–“åºåˆ—\n",
    "y_gender = []   # å°æ‡‰æ€§åˆ¥\n",
    "y_hand = []  # å°æ‡‰æ…£ç”¨æ‰‹\n",
    "y_years = []  # å°æ‡‰çƒé½¡ï¼ˆ0,1,2ï¼‰\n",
    "y_level = []  # å°æ‡‰ç­‰ç´šï¼ˆ0~3ï¼‰\n",
    "\n",
    "# é–‹å§‹è·‘å…¨éƒ¨çš„ txt æª”\n",
    "for _, row in tqdm(info_df.iterrows(), total=len(info_df)):\n",
    "    uid = row['unique_id']\n",
    "    file_path = os.path.join(DATA_DIR, f\"{uid}.txt\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}\")\n",
    "        continue  # è‹¥æª”æ¡ˆä¸å­˜åœ¨å°±è·³é\n",
    "\n",
    "    # è™•ç† .txt æª” â†’ è½‰æˆ (2048, 6) çš„é™£åˆ—\n",
    "    x = load_and_process_txt(file_path)  # ğŸ‘ˆ è®€ + æ¨™æº–åŒ– + è£œé•·åº¦\n",
    "    X_data.append(x)  # æ”¾å…¥è¨“ç·´è³‡æ–™é›†\n",
    "\n",
    "    # åŠ å…¥å°æ‡‰çš„æ¨™ç±¤\n",
    "    y_gender.append(row['gender'])\n",
    "    y_hand.append(row['hand'])\n",
    "    y_years.append(row['play years'])  # ä¸ç”¨è™•ç†ï¼Œå·²ç¶“æ˜¯ 0,1,2\n",
    "    y_level.append(row['level'])       # å·²ç¶“è½‰ç‚º 0~3\n",
    "\n",
    "# è½‰æˆ numpy é™£åˆ—\n",
    "X_data = np.array(X_data)\n",
    "y_gender = np.array(y_gender)\n",
    "y_hand = np.array(y_hand)\n",
    "y_years = np.array(y_years)\n",
    "y_level = np.array(y_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ebd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­˜ä¸‹ä¾†ï¼Œä¹‹å¾Œå¤§å®¶å¯ä»¥ç›´æ¥ç”¨\n",
    "# å¦‚æœä½ è³‡æ–™é è™•ç†æœ‰æ›´å‹•çš„è©±ï¼Œä¸‹é¢çš„æª”åä¹Ÿè¨˜å¾—æ”¹ï¼ï¼ï¼\n",
    "# æ›´æ”¹æ–¹æ¡ˆï¼Œé€™äº›æª”æ¡ˆå°±è‡ªå·±å¯ä»¥å­˜ä¸‹ä¾†ï¼Œä¸è¦ä¸Šå‚³åˆ°githubä¸Š\n",
    "np.save(\"X_data.npy\", X_data)\n",
    "np.save(\"y_gender.npy\", y_gender)\n",
    "np.save(\"y_hand.npy\", y_hand)\n",
    "np.save(\"y_years.npy\", y_years)\n",
    "np.save(\"y_level.npy\", y_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8cffa",
   "metadata": {},
   "source": [
    "## Test\n",
    "çœ‹ä¸€ä¸‹è³‡æ–™åˆ†å¸ƒè€Œå·²ï¼Œä¹Ÿå¯ä»¥è·³é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aaf175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # è™•ç†æ•¸å€¼è³‡æ–™\n",
    "X_data = np.load(\"X_data.npy\")\n",
    "y_gender = np.load(\"y_gender.npy\")\n",
    "y_hand = np.load(\"y_hand.npy\")\n",
    "y_years = np.load(\"y_years.npy\")\n",
    "y_level = np.load(\"y_level.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d9c98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”·ç”Ÿï¼ˆ0ï¼‰æœ‰å¹¾ç­†ï¼š 1627\n",
      "å¥³ç”Ÿï¼ˆ1ï¼‰æœ‰å¹¾ç­†ï¼š 328\n"
     ]
    }
   ],
   "source": [
    "print(\"ç”·ç”Ÿï¼ˆ0ï¼‰æœ‰å¹¾ç­†ï¼š\", np.sum(y_gender == 0))\n",
    "print(\"å¥³ç”Ÿï¼ˆ1ï¼‰æœ‰å¹¾ç­†ï¼š\", np.sum(y_gender == 1))\n",
    "# å¹¾ä¹ 5 å€å·®è· åš´é‡å¤±è¡¡\n",
    "# å»ºè­°ä½¿ç”¨ oversampling / SMOTE ç­‰æŠ€å·§\n",
    "\n",
    "# W1 ä½¿ç”¨ class_weightï¼ˆæ¨è–¦ï¼ï¼‰\n",
    "# W2 ä¸Šæ¡æ¨£å°‘æ•¸é¡åˆ¥ï¼ˆå¦‚å¥³ç”Ÿï¼‰\n",
    "# W3 è¨­è¨ˆå¤šç›®æ¨™æå¤±æ™‚ï¼Œå° gender ç‰¹åˆ¥èª¿æ•´æ¬Šé‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b1f0e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å³æ‰‹ï¼ˆ0ï¼‰æœ‰å¹¾ç­†ï¼š 1589\n",
      "å·¦æ‰‹ï¼ˆ1ï¼‰æœ‰å¹¾ç­†ï¼š 366\n",
      "ä½çƒé½¡æœ‰å¹¾ç­†ï¼š 387\n",
      "ä¸­çƒé½¡æœ‰å¹¾ç­†ï¼š 868\n",
      "é«˜çƒé½¡æœ‰å¹¾ç­†ï¼š 700\n",
      "ç­‰ç´š0ï¼š 715\n",
      "ç­‰ç´š1ï¼š 201\n",
      "ç­‰ç´š2ï¼š 136\n",
      "ç­‰ç´š3ï¼š 903\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥æŒæ‹æ‰‹åˆ†å¸ƒ\n",
    "print(\"å³æ‰‹ï¼ˆ0ï¼‰æœ‰å¹¾ç­†ï¼š\", np.sum(y_hand == 0))\n",
    "print(\"å·¦æ‰‹ï¼ˆ1ï¼‰æœ‰å¹¾ç­†ï¼š\", np.sum(y_hand == 1))\n",
    "# 4.34 å€å·®è·â†’ æ˜é¡¯ä¸å¹³è¡¡\n",
    "\n",
    "# æª¢æŸ¥çƒé½¡åˆ†å¸ƒ\n",
    "print(\"ä½çƒé½¡æœ‰å¹¾ç­†ï¼š\", np.sum(y_years == 0))\n",
    "print(\"ä¸­çƒé½¡æœ‰å¹¾ç­†ï¼š\", np.sum(y_years == 1))\n",
    "print(\"é«˜çƒé½¡æœ‰å¹¾ç­†ï¼š\", np.sum(y_years == 2))\n",
    "# å·® â‰ˆ 2.2 å€ â†’ è¼•åº¦ä¸å¹³è¡¡\n",
    "\n",
    "# æª¢æŸ¥ç­‰ç´šåˆ†å¸ƒ\n",
    "print(\"ç­‰ç´š0ï¼š\", np.sum(y_level == 0))\n",
    "print(\"ç­‰ç´š1ï¼š\", np.sum(y_level == 1))\n",
    "print(\"ç­‰ç´š2ï¼š\", np.sum(y_level == 2))\n",
    "print(\"ç­‰ç´š3ï¼š\", np.sum(y_level == 3))\n",
    "# 6.6 å€å·®è· â†’ æ˜é¡¯ä¸å¹³è¡¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec53cb4",
   "metadata": {},
   "source": [
    "## 0. è¼‰è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # è™•ç†æ•¸å€¼è³‡æ–™\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.load(\"X_data.npy\")\n",
    "y_gender = np.load(\"y_gender.npy\")\n",
    "y_hand = np.load(\"y_hand.npy\")\n",
    "y_years = np.load(\"y_years.npy\")\n",
    "y_level = np.load(\"y_level.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰è³‡æ–™ï¼ˆ80% è¨“ç·´, 20% é©—è­‰ï¼‰\n",
    "X_train, X_val, y_gender_train, y_gender_val, y_hand_train, y_hand_val, y_years_train, y_years_val, y_level_train, y_level_val = train_test_split(\n",
    "    X_data, y_gender, y_hand, y_years, y_level, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075be328",
   "metadata": {},
   "source": [
    "## Optional. æ¬Šé‡è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd28983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights_years: {0: np.float64(1.6838931955211025), 1: np.float64(0.750768049155146), 2: np.float64(0.930952380952381)}\n",
      "class weights_gender: {0: np.float64(0.60079901659496), 1: np.float64(2.980182926829268)}\n",
      "class weights_hands: {0: np.float64(0.6151667715544368), 1: np.float64(2.670765027322404)}\n",
      "class weights_level: {0: np.float64(0.6835664335664335), 1: np.float64(2.431592039800995), 2: np.float64(3.59375), 3: np.float64(0.54125138427464)}\n"
     ]
    }
   ],
   "source": [
    "# è™•ç†çƒé½¡è³‡æ–™ä¸å¹³è¡¡\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "weights_years = compute_class_weight(class_weight='balanced', classes=np.unique(y_years), y=y_years)\n",
    "weights_gender = compute_class_weight(class_weight='balanced', classes=np.unique(y_gender), y=y_gender)\n",
    "weights_hands = compute_class_weight(class_weight='balanced', classes=np.unique(y_hand), y=y_hand)\n",
    "weights_level = compute_class_weight(class_weight='balanced', classes=np.unique(y_level), y=y_level)\n",
    "print(\"class weights_years:\", dict(enumerate(weights_years)))\n",
    "print(\"class weights_gender:\", dict(enumerate(weights_gender)))\n",
    "print(\"class weights_hands:\", dict(enumerate(weights_hands)))\n",
    "print(\"class weights_level:\", dict(enumerate(weights_level)))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pos_weight_gender = torch.tensor([weights_gender[1]], dtype=torch.float32)\n",
    "pos_weight_hand   = torch.tensor([weights_hands[1]], dtype=torch.float32)\n",
    "ce_weight_years   = torch.tensor(weights_years, dtype=torch.float32)\n",
    "ce_weight_level   = torch.tensor(weights_level, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993ccff",
   "metadata": {},
   "source": [
    "## 1. å®šç¾©è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75df97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# è‡ªå®šç¾©ä¸€å€‹èƒ½è¢« DataLoader ä½¿ç”¨çš„è³‡æ–™é¡åˆ¥\n",
    "# ç•¶ DataLoader åš batch æ™‚æœƒå‘¼å«é€™å€‹å‡½å¼\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TableTennisDataset(Dataset):\n",
    "    def __init__(self, X, y_gender, y_hand, y_years, y_level):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_gender = torch.tensor(y_gender, dtype=torch.float32)\n",
    "        self.y_hand = torch.tensor(y_hand, dtype=torch.float32)\n",
    "        self.y_years = torch.tensor(y_years, dtype=torch.long)\n",
    "        self.y_level = torch.tensor(y_level, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], {\n",
    "            'gender': self.y_gender[idx],\n",
    "            'hand': self.y_hand[idx],\n",
    "            'years': self.y_years[idx],\n",
    "            'level': self.y_level[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a547ab2",
   "metadata": {},
   "source": [
    "## 2. å®šç¾©æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c48481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å®šç¾©æå¤±å‡½æ•¸\n",
    "# è¦ä¸è¦åŠ æ¬Š\n",
    "use_class_weight = False\n",
    "# use_class_weight = True\n",
    "\n",
    "if use_class_weight:\n",
    "    # binary åˆ†é¡ï¼šæ€§åˆ¥ & æ‰‹ â†’ BCEWithLogitsLoss + pos_weight\n",
    "    loss_gender = nn.BCEWithLogitsLoss(pos_weight=pos_weight_gender)\n",
    "    loss_hand   = nn.BCEWithLogitsLoss(pos_weight=pos_weight_hand)\n",
    "\n",
    "    # å¤šé¡åˆ¥åˆ†é¡ï¼šçƒé½¡ & ç­‰ç´š â†’ CrossEntropyLoss + weight\n",
    "    loss_years = nn.CrossEntropyLoss(weight=ce_weight_years)\n",
    "    loss_level = nn.CrossEntropyLoss(weight=ce_weight_level)\n",
    "else:\n",
    "    loss_gender = nn.BCEWithLogitsLoss()\n",
    "    loss_hand   = nn.BCEWithLogitsLoss()\n",
    "    loss_years  = nn.CrossEntropyLoss()\n",
    "    loss_level  = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905e56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹\n",
    "class MultiTaskCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskCNN, self).__init__()\n",
    "\n",
    "        # 1D CNN feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=6, out_channels=64, kernel_size=5, padding=2),  # (B, 64, 2048)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),  # (B, 64, 1024)\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),  # (B, 128, 1024)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # (B, 128, 512)\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),  # (B, 256, 512)\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)  # (B, 256, 1)\n",
    "        )\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # é æ¸¬ headï¼ˆå…±ç”¨ CNN ç‰¹å¾µï¼‰\n",
    "        self.gender_head = nn.Linear(256, 1)   # binary\n",
    "        self.hand_head   = nn.Linear(256, 1)   # binary\n",
    "        self.years_head  = nn.Linear(256, 3)   # 3 é¡\n",
    "        self.level_head  = nn.Linear(256, 4)   # 4 é¡\n",
    "\n",
    "    def forward(self, x):  # x: (B, 2048, 6)\n",
    "        x = x.permute(0, 2, 1)  # è½‰æˆ (B, 6, 2048) â†’ CNN è¦æ±‚çš„æ ¼å¼\n",
    "        feat = self.feature_extractor(x)  # (B, 256, 1)\n",
    "        feat = self.flatten(feat)  # (B, 256)\n",
    "\n",
    "        return {\n",
    "            'gender': self.gender_head(feat).squeeze(-1),  # (B,)\n",
    "            'hand': self.hand_head(feat).squeeze(-1),\n",
    "            'years': self.years_head(feat),  # (B, 3)\n",
    "            'level': self.level_head(feat)   # (B, 4)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f909d93",
   "metadata": {},
   "source": [
    "## 3. å®šç¾©æ¨¡å‹è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e53451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_gender, correct_hand, correct_years, correct_level = 0, 0, 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for x, y in tqdm(dataloader):\n",
    "        x = x.to(device)\n",
    "        y_gender = y['gender'].to(device)\n",
    "        y_hand = y['hand'].to(device)\n",
    "        y_years = y['years'].to(device)\n",
    "        y_level = y['level'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "\n",
    "        # è¨ˆç®— loss\n",
    "        loss_g = loss_gender(out['gender'], y_gender)\n",
    "        loss_h = loss_hand(out['hand'], y_hand)\n",
    "        loss_y = loss_years(out['years'], y_years)\n",
    "        loss_l = loss_level(out['level'], y_level)\n",
    "        loss = loss_g + loss_h + loss_y + loss_l\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # â• æº–ç¢ºç‡è¨ˆç®—\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        # gender é æ¸¬ï¼ˆ>0 â†’ 1, å¦å‰‡ 0ï¼‰\n",
    "        pred_gender = (torch.sigmoid(out['gender']) > 0.5).float()\n",
    "        correct_gender += (pred_gender == y_gender).sum().item()\n",
    "\n",
    "        # hand é æ¸¬\n",
    "        pred_hand = (torch.sigmoid(out['hand']) > 0.5).float()\n",
    "        correct_hand += (pred_hand == y_hand).sum().item()\n",
    "\n",
    "        # years é æ¸¬\n",
    "        pred_years = torch.argmax(out['years'], dim=1)\n",
    "        correct_years += (pred_years == y_years).sum().item()\n",
    "\n",
    "        # level é æ¸¬\n",
    "        pred_level = torch.argmax(out['level'], dim=1)\n",
    "        correct_level += (pred_level == y_level).sum().item()\n",
    "\n",
    "    acc_gender = correct_gender / total_samples\n",
    "    acc_hand = correct_hand / total_samples\n",
    "    acc_years = correct_years / total_samples\n",
    "    acc_level = correct_level / total_samples\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Loss: {avg_loss:.4f} | acc_gender: {acc_gender:.3f} | acc_hand: {acc_hand:.3f} | acc_years: {acc_years:.3f} | acc_level: {acc_level:.3f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964ed83",
   "metadata": {},
   "source": [
    "## 4. å®šç¾©é©—è­‰å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35f8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_gender, correct_hand, correct_years, correct_level = 0, 0, 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y_gender = y['gender'].to(device)\n",
    "            y_hand = y['hand'].to(device)\n",
    "            y_years = y['years'].to(device)\n",
    "            y_level = y['level'].to(device)\n",
    "\n",
    "            out = model(x)\n",
    "\n",
    "            loss_g = loss_gender(out['gender'], y_gender)\n",
    "            loss_h = loss_hand(out['hand'], y_hand)\n",
    "            loss_y = loss_years(out['years'], y_years)\n",
    "            loss_l = loss_level(out['level'], y_level)\n",
    "            loss = loss_g + loss_h + loss_y + loss_l\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            total_samples += x.size(0)\n",
    "\n",
    "            pred_gender = (torch.sigmoid(out['gender']) > 0.5).float()\n",
    "            correct_gender += (pred_gender == y_gender).sum().item()\n",
    "\n",
    "            pred_hand = (torch.sigmoid(out['hand']) > 0.5).float()\n",
    "            correct_hand += (pred_hand == y_hand).sum().item()\n",
    "\n",
    "            pred_years = torch.argmax(out['years'], dim=1)\n",
    "            correct_years += (pred_years == y_years).sum().item()\n",
    "\n",
    "            pred_level = torch.argmax(out['level'], dim=1)\n",
    "            correct_level += (pred_level == y_level).sum().item()\n",
    "\n",
    "    acc_gender = correct_gender / total_samples\n",
    "    acc_hand = correct_hand / total_samples\n",
    "    acc_years = correct_years / total_samples\n",
    "    acc_level = correct_level / total_samples\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"\\n[Validation] Loss: {avg_loss:.4f} | acc_gender: {acc_gender:.3f} | acc_hand: {acc_hand:.3f} | acc_years: {acc_years:.3f} | acc_level: {acc_level:.3f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ac21a",
   "metadata": {},
   "source": [
    "## ä¸»ç¨‹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:19<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.9471 | acc_gender: 0.810 | acc_hand: 0.813 | acc_years: 0.554 | acc_level: 0.559\n",
      "\n",
      "[Validation] Loss: 2.5980 | acc_gender: 0.859 | acc_hand: 0.813 | acc_years: 0.563 | acc_level: 0.614\n",
      "\n",
      "ğŸŸ¢ Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:20<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2027 | acc_gender: 0.826 | acc_hand: 0.964 | acc_years: 0.636 | acc_level: 0.662\n",
      "\n",
      "[Validation] Loss: 2.2410 | acc_gender: 0.872 | acc_hand: 0.985 | acc_years: 0.573 | acc_level: 0.637\n",
      "\n",
      "ğŸŸ¢ Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:21<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9364 | acc_gender: 0.871 | acc_hand: 0.985 | acc_years: 0.646 | acc_level: 0.689\n",
      "\n",
      "[Validation] Loss: 1.9613 | acc_gender: 0.905 | acc_hand: 0.990 | acc_years: 0.621 | acc_level: 0.673\n",
      "\n",
      "ğŸŸ¢ Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:19<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.7146 | acc_gender: 0.896 | acc_hand: 0.997 | acc_years: 0.707 | acc_level: 0.715\n",
      "\n",
      "[Validation] Loss: 1.7173 | acc_gender: 0.949 | acc_hand: 0.992 | acc_years: 0.657 | acc_level: 0.708\n",
      "\n",
      "ğŸŸ¢ Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:17<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4520 | acc_gender: 0.926 | acc_hand: 0.999 | acc_years: 0.728 | acc_level: 0.776\n",
      "\n",
      "[Validation] Loss: 1.5160 | acc_gender: 0.957 | acc_hand: 0.995 | acc_years: 0.721 | acc_level: 0.783\n",
      "\n",
      "ğŸŸ¢ Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:16<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2003 | acc_gender: 0.940 | acc_hand: 1.000 | acc_years: 0.793 | acc_level: 0.827\n",
      "\n",
      "[Validation] Loss: 1.2413 | acc_gender: 0.951 | acc_hand: 0.995 | acc_years: 0.829 | acc_level: 0.808\n",
      "\n",
      "ğŸŸ¢ Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:17<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0356 | acc_gender: 0.944 | acc_hand: 1.000 | acc_years: 0.813 | acc_level: 0.857\n",
      "\n",
      "[Validation] Loss: 1.0716 | acc_gender: 0.962 | acc_hand: 0.992 | acc_years: 0.844 | acc_level: 0.849\n",
      "\n",
      "ğŸŸ¢ Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:18<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8832 | acc_gender: 0.949 | acc_hand: 1.000 | acc_years: 0.862 | acc_level: 0.886\n",
      "\n",
      "[Validation] Loss: 0.9649 | acc_gender: 0.962 | acc_hand: 0.992 | acc_years: 0.834 | acc_level: 0.885\n",
      "\n",
      "ğŸŸ¢ Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:20<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7980 | acc_gender: 0.954 | acc_hand: 1.000 | acc_years: 0.859 | acc_level: 0.898\n",
      "\n",
      "[Validation] Loss: 0.8241 | acc_gender: 0.967 | acc_hand: 0.992 | acc_years: 0.885 | acc_level: 0.867\n",
      "\n",
      "ğŸŸ¢ Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:19<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7448 | acc_gender: 0.958 | acc_hand: 1.000 | acc_years: 0.875 | acc_level: 0.900\n",
      "\n",
      "[Validation] Loss: 0.7340 | acc_gender: 0.969 | acc_hand: 0.992 | acc_years: 0.854 | acc_level: 0.910\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # è®“ä½ èƒ½è‡ªå‹•ä½¿ç”¨ GPUï¼ˆæœ‰å°±ç”¨ï¼Œæ²’æœ‰å°± CPUï¼‰\n",
    "model = MultiTaskCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # å­¸ç¿’ç‡learning_rateç‚º 1e-3ï¼ˆé è¨­å¾ˆå¥½ç”¨ï¼‰\n",
    "\n",
    "# å»ºç«‹ Dataset ç‰©ä»¶\n",
    "dataset_train = TableTennisDataset(X_train, y_gender_train, y_hand_train, y_years_train, y_level_train)\n",
    "dataset_val = TableTennisDataset(X_val, y_gender_val, y_hand_val, y_years_val, y_level_val)\n",
    "\n",
    "\n",
    "# å»ºç«‹ DataLoaderï¼ˆè¨“ç·´èˆ‡é©—è­‰ï¼‰\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32, shuffle=False)\n",
    "\n",
    "# ä¿®æ”¹è¨“ç·´æµç¨‹ï¼šæ¯ä¸€è¼ªå¾ŒåŠ ä¸Šé©—è­‰\n",
    "for epoch in range(10):\n",
    "    print(f\"\\nğŸŸ¢ Epoch {epoch+1}\")\n",
    "    train_loss = train_one_epoch(model, dataloader_train, optimizer, device)\n",
    "    val_loss = evaluate_model(model, dataloader_val, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24d696",
   "metadata": {},
   "source": [
    "## å„²å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f648120",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'modelNoWeight042400.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03d23f",
   "metadata": {},
   "source": [
    "## æ¸¬è³‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "070a177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 2048\n",
    "# 1. è¼‰å…¥æ¸¬è©¦è³‡æ–™\n",
    "def load_test_data(test_info_path, test_data_dir, seq_len=SEQ_LEN):\n",
    "    test_info = pd.read_csv(test_info_path)\n",
    "    X_test = []\n",
    "    uids = []\n",
    "\n",
    "    for _, row in tqdm(test_info.iterrows(), total=len(test_info)):\n",
    "        uid = row['unique_id']\n",
    "        file_path = os.path.join(test_data_dir, f\"{uid}.txt\")\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        x = load_and_process_txt(file_path, seq_len)\n",
    "        X_test.append(x)\n",
    "        uids.append(uid)\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    return uids, torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5540113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. é æ¸¬å‡½å¼\n",
    "def predict(model, X_test_tensor, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(X_test_tensor):\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            out = model(x)\n",
    "\n",
    "            gender = torch.sigmoid(out['gender']).item()\n",
    "            hand = torch.sigmoid(out['hand']).item()\n",
    "            years = torch.softmax(out['years'], dim=1).squeeze().cpu().numpy()\n",
    "            level = torch.softmax(out['level'], dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            preds.append([gender, hand] + years.tolist() + level.tolist())\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99722fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_txt(file_path, seq_len=SEQ_LEN):\n",
    "    data = np.loadtxt(file_path)  # æŠŠ.txtè½‰ç‚ºnumpyé™£åˆ— (T, 6)\n",
    "    \n",
    "    # æ¨™æº–åŒ–è™•ç†ï¼šè®“æ¯å€‹ç¶­åº¦çš„å¹³å‡è®Š0ï¼Œæ¨™æº–å·®è®Š1\n",
    "    data = (data - data.mean(axis=0)) / (data.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # è£åˆ‡æˆ–è£œé›¶ï¼šçµ±ä¸€é•·åº¦ç‚º SEQ_LEN\n",
    "    if data.shape[0] >= seq_len:\n",
    "        data = data[:seq_len]  # è£åˆ‡\n",
    "    else:\n",
    "        pad_width = seq_len - data.shape[0]\n",
    "        data = np.pad(data, ((0, pad_width), (0, 0)), mode='constant')  # np.pad() æ˜¯ NumPy çš„è£œé›¶å‡½å¼\n",
    "        # (0, pad_width) è¡¨ç¤ºå°åˆ—æ•¸ï¼ˆæ™‚é–“é»ï¼‰è£œé›¶åœ¨ã€Œå¾Œé¢ã€\n",
    "        # (0, 0) è¡¨ç¤ºæ¬„ä½æ•¸ï¼ˆ6 å€‹æ„Ÿæ¸¬å™¨ç¶­åº¦ï¼‰ä¸è£œé›¶\n",
    "        # mode='constant' ç”¨ 0 å»è£œ\n",
    "    \n",
    "    return data  # shape = (2048, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9059a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "def create_submission(uids, preds, save_path='submission.csv'):\n",
    "    def format_float(val):\n",
    "        return str(Decimal(val).quantize(Decimal('0.000001'), rounding=ROUND_HALF_UP))\n",
    "\n",
    "    df = pd.DataFrame(preds, columns=[\n",
    "        'gender', 'hold racket handed',\n",
    "        'play years_0', 'play years_1', 'play years_2',\n",
    "        'level_2', 'level_3', 'level_4', 'level_5'\n",
    "    ])\n",
    "    df.insert(0, 'unique_id', uids)\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = df[col].apply(format_float)\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\", newline='\\n') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print(f\"âœ”ï¸ Submission saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e78720ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1430 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1430/1430 [00:10<00:00, 141.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1430/1430 [00:08<00:00, 170.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Submission saved to submission042402.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ä¿®æ”¹æˆä½ è‡ªå·±çš„ test æª”æ¡ˆè·¯å¾‘\n",
    "TEST_INFO = \"39_Test_Dataset/39_Test_Dataset/test_info.csv\"\n",
    "TEST_DATA = \"39_Test_Dataset/39_Test_Dataset/test_data\"\n",
    "\n",
    "# 1. è¼‰å…¥ test data\n",
    "uids, X_test_tensor = load_test_data(TEST_INFO, TEST_DATA)\n",
    "\n",
    "\n",
    "model = MultiTaskCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"modelNoWeight042400.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 2. é æ¸¬\n",
    "preds = predict(model, X_test_tensor, device)\n",
    "\n",
    "submissionCSV = \"submission042402.csv\" # è¨˜å¾—æ”¹\n",
    "# 3. ç”¢å‡º CSV\n",
    "create_submission(uids, preds, save_path=submissionCSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
